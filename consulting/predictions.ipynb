{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict test dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GBMs + TomekLinks is our choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'cons_12m', 'cons_gas_12m', 'cons_last_month',\n",
       "       'cons_last_month.1', 'forecast_cons_12m', 'forecast_cons_year',\n",
       "       'forecast_discount_energy', 'forecast_meter_rent_12m',\n",
       "       'forecast_price_energy_p1', 'forecast_price_energy_p2',\n",
       "       'forecast_price_pow_p1', 'has_gas', 'imp_cons', 'margin_gross_pow_ele',\n",
       "       'margin_net_pow_ele', 'nb_prod_act', 'net_margin', 'num_years_antig',\n",
       "       'pow_max', 'channel0', 'channel1', 'channel2', 'channel3', 'channel4',\n",
       "       'channel5', 'channel6', 'date_activ', 'date_end', 'date_modif_prod',\n",
       "       'date_renewal', 'origin0', 'origin1', 'origin2', 'origin3', 'origin4',\n",
       "       'origin5', 'price_p1_var', 'price_p2_var', 'price_p3_var',\n",
       "       'price_p1_fix', 'price_p2_fix', 'price_p3_fix', 'churn'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_df = pd.read_csv('./all_train.csv')\n",
    "train_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed:  0\n",
      "AUROC score test/train:  0.7011243960545943 / 0.7671988956541802\n",
      "Brier loss test/train:  0.08286915252757507 / 0.0806272106290579\n",
      "[tn, fp, fn, tp] test/train:  [4077    6  433    7] / [9471    0 1037   43]\n",
      "Seed:  1\n",
      "AUROC score test/train:  0.6614384958388898 / 0.7754891824579926\n",
      "Brier loss test/train:  0.08866151920317585 / 0.078152420396888\n",
      "[tn, fp, fn, tp] test/train:  [4047   14  456    6] / [9493    0 1014   44]\n",
      "Seed:  2\n",
      "AUROC score test/train:  0.6852731774330681 / 0.7676016166946577\n",
      "Brier loss test/train:  0.08498756292626564 / 0.08005418314464756\n",
      "[tn, fp, fn, tp] test/train:  [4071    6  442    4] / [9477    0 1031   43]\n",
      "Seed:  3\n",
      "AUROC score test/train:  0.670845058479997 / 0.7799518480218162\n",
      "Brier loss test/train:  0.08934021512556156 / 0.07734221884402628\n",
      "[tn, fp, fn, tp] test/train:  [4046   12  461    4] / [9496    0 1009   46]\n",
      "Seed:  4\n",
      "AUROC score test/train:  0.6866511460512406 / 0.7770389490010817\n",
      "Brier loss test/train:  0.08239523145415585 / 0.07998333078435317\n",
      "[tn, fp, fn, tp] test/train:  [4086    2  427    8] / [9465    1 1041   44]\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import TomekLinks \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import brier_score_loss\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import pandas as pd\n",
    "\n",
    "train_df = pd.read_csv('./all_train.csv')\n",
    "train_df = train_df.drop(['cons_last_month.1'], axis=1)\n",
    "\n",
    "# We don't need ids here so just drop it for a while\n",
    "ids = list(train_df.id)\n",
    "train_gbm = train_df.drop(['id'], axis=1)\n",
    "\n",
    "X = train_gbm.drop(['churn'], axis=1).as_matrix()\n",
    "y = train_gbm['churn'].as_matrix()\n",
    "\n",
    "def oversample(X, y, seed):\n",
    "    tl = TomekLinks()\n",
    "    X_re, y_re = tl.fit_sample(X, y)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_re, y_re, test_size=0.3, random_state=seed)\n",
    "    #X_train_re, y_train_re = SMOTE().fit_sample(X_train, y_train)\n",
    "    #X_test_re, y_test_re = SMOTE().fit_sample(X_test, y_test)\n",
    "\n",
    "    gbm = GradientBoostingClassifier(n_estimators=100, \\\n",
    "                                     learning_rate=0.2, \\\n",
    "                                     max_depth=2, \\\n",
    "                                     min_samples_leaf=1)\n",
    "    #gbm.fit(X_train_re, y_train_re)\n",
    "    #y_pred = gbm.predict_proba(X_test_re)\n",
    "    gbm.fit(X_train, y_train)\n",
    "    y_pred = gbm.predict_proba(X_test)\n",
    "    y_positive = [x[1] for x in y_pred]\n",
    "    y_binary = [0 if x[0] > x[1] else 1 for x in y_pred]\n",
    "    \n",
    "    y_train_pred = gbm.predict_proba(X_train)\n",
    "    y_train_positive = [x[1] for x in y_train_pred]\n",
    "    y_train_binary = [0 if x[0] > x[1] else 1 for x in y_train_pred]\n",
    "\n",
    "    print (\"Seed: \", seed)\n",
    "    auroc_gbm = roc_auc_score(y_test, y_positive)\n",
    "    brier_gbm = brier_score_loss(y_test, y_positive)\n",
    "    confusion = confusion_matrix(y_test, y_binary).ravel()\n",
    "    \n",
    "    #auroc_gbm = roc_auc_score(y_test_re, y_positive)\n",
    "    #brier_gbm = brier_score_loss(y_test_re, y_positive)\n",
    "    #confusion = confusion_matrix(y_test_re, y_binary).ravel()\n",
    "    \n",
    "    auroc_gbm_t = roc_auc_score(y_train, y_train_positive)\n",
    "    brier_gbm_t = brier_score_loss(y_train, y_train_positive)\n",
    "    confusion_t = confusion_matrix(y_train, y_train_binary).ravel()\n",
    "    \n",
    "    \n",
    "    print (\"AUROC score test/train: \", auroc_gbm, '/', auroc_gbm_t)\n",
    "    print (\"Brier loss test/train: \", brier_gbm, '/', brier_gbm_t)\n",
    "    print (\"[tn, fp, fn, tp] test/train: \", confusion, '/', confusion_t)\n",
    "    return gbm\n",
    "    \n",
    "gbms = []\n",
    "for i in range (5):\n",
    "    gbms.append(oversample(X, y, i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed:  0\n",
      "AUROC score test/train:  0.6578354869385243 / 0.9811468705527123\n",
      "Brier loss test/train:  0.08856345258518972 / 0.04581888391345607\n",
      "[tn, fp, fn, tp] test/train:  [2791   41  274   29] / [11254    68  1117 10205]\n",
      "Seed:  1\n",
      "AUROC score test/train:  0.6922265562331937 / 0.981333787202677\n",
      "Brier loss test/train:  0.08400012580079003 / 0.04615288946121129\n",
      "[tn, fp, fn, tp] test/train:  [2799   42  257   37] / [11239    74  1137 10176]\n",
      "Seed:  2\n",
      "AUROC score test/train:  0.6920888465410614 / 0.9799001159972154\n",
      "Brier loss test/train:  0.07981473574970342 / 0.04787652984528908\n",
      "[tn, fp, fn, tp] test/train:  [2823   40  229   43] / [11217    74  1201 10090]\n",
      "Seed:  3\n",
      "AUROC score test/train:  0.7027364788596123 / 0.981237965031585\n",
      "Brier loss test/train:  0.08597355014066746 / 0.046168228434464664\n",
      "[tn, fp, fn, tp] test/train:  [2791   37  273   34] / [11269    57  1167 10159]\n",
      "Seed:  4\n",
      "AUROC score test/train:  0.6595377307627954 / 0.9813758102126078\n",
      "Brier loss test/train:  0.08927387782022077 / 0.04598959251441539\n",
      "[tn, fp, fn, tp] test/train:  [2784   47  270   34] / [11242    81  1133 10190]\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import brier_score_loss\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import pandas as pd\n",
    "\n",
    "train_df = pd.read_csv('./all_train.csv')\n",
    "train_df = train_df.drop(['cons_last_month.1'], axis=1)\n",
    "\n",
    "# We don't need ids here so just drop it for a while\n",
    "ids = list(train_df.id)\n",
    "train_gbm = train_df.drop(['id'], axis=1)\n",
    "\n",
    "X = train_gbm.drop(['churn'], axis=1).as_matrix()\n",
    "y = train_gbm['churn'].as_matrix()\n",
    "\n",
    "def oversample(X, y, seed):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=seed)\n",
    "    X_train_re, y_train_re = SMOTE().fit_sample(X_train, y_train)\n",
    "    #X_test_re, y_test_re = SMOTE().fit_sample(X_test, y_test)\n",
    "\n",
    "    gbm = GradientBoostingClassifier(n_estimators=200, \\\n",
    "                                     learning_rate=0.2, \\\n",
    "                                     max_depth=3, \\\n",
    "                                     min_samples_leaf=1)\n",
    "    #gbm.fit(X_train_re, y_train_re)\n",
    "    #y_pred = gbm.predict_proba(X_test_re)\n",
    "    gbm.fit(X_train_re, y_train_re)\n",
    "    y_pred = gbm.predict_proba(X_test)\n",
    "    y_positive = [x[1] for x in y_pred]\n",
    "    y_binary = [0 if x[0] > x[1] else 1 for x in y_pred]\n",
    "    \n",
    "    y_train_pred = gbm.predict_proba(X_train_re)\n",
    "    y_train_positive = [x[1] for x in y_train_pred]\n",
    "    y_train_binary = [0 if x[0] > x[1] else 1 for x in y_train_pred]\n",
    "\n",
    "    print (\"Seed: \", seed)\n",
    "    auroc_gbm = roc_auc_score(y_test, y_positive)\n",
    "    brier_gbm = brier_score_loss(y_test, y_positive)\n",
    "    confusion = confusion_matrix(y_test, y_binary).ravel()\n",
    "    \n",
    "    #auroc_gbm = roc_auc_score(y_test_re, y_positive)\n",
    "    #brier_gbm = brier_score_loss(y_test_re, y_positive)\n",
    "    #confusion = confusion_matrix(y_test_re, y_binary).ravel()\n",
    "    \n",
    "    auroc_gbm_t = roc_auc_score(y_train_re, y_train_positive)\n",
    "    brier_gbm_t = brier_score_loss(y_train_re, y_train_positive)\n",
    "    confusion_t = confusion_matrix(y_train_re, y_train_binary).ravel()\n",
    "    \n",
    "    \n",
    "    print (\"AUROC score test/train: \", auroc_gbm, '/', auroc_gbm_t)\n",
    "    print (\"Brier loss test/train: \", brier_gbm, '/', brier_gbm_t)\n",
    "    print (\"[tn, fp, fn, tp] test/train: \", confusion, '/', confusion_t)\n",
    "    return gbm\n",
    "    \n",
    "gbms = []\n",
    "for i in range (5):\n",
    "    gbms.append(oversample(X, y, i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_raw = pd.read_csv('./ml_case_data/ml_case_test_data.csv')\n",
    "id_raw = list(test_raw.id.unique())\n",
    "X_test = pd.read_csv('./all_test.csv')\n",
    "test_id = list(X_test.id)\n",
    "\n",
    "id_todo = [x for x in id_raw if x not in test_id]\n",
    "test_todo = test_raw[test_raw.id.isin(id_todo)] \\\n",
    "                    .drop(['activity_new', 'campaign_disc_ele', 'date_first_activ', 'forecast_base_bill_ele', 'forecast_base_bill_year', 'forecast_bill_12m', 'forecast_cons'], axis=1) \\\n",
    "                    .fillna('nodata')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_hist = pd.read_csv('./ml_case_data/ml_case_test_hist_data.csv')\n",
    "test_hist_todo = test_hist[test_hist.id.isin(id_todo)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/admin/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:33: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv('./all_train.csv')\n",
    "\n",
    "user = pd.read_csv('./preprocessed1_train.csv')\n",
    "user_test = pd.read_csv('./preprocessed1_test.csv')\n",
    "origin_up_ = list(user_test.origin_up.unique())\n",
    "origin_up = list(set(list(user.origin_up.unique()) + origin_up_ ))\n",
    "origins_cols = ['origin'+str(i) for i in range(len(origin_up))]\n",
    "\n",
    "channel = list(user.channel_sales.unique())\n",
    "chan_cols = ['channel'+str(i) for i in range(len(channel))]\n",
    "\n",
    "from datetime import datetime\n",
    "start_date = '2000-07-25'\n",
    "FMT = '%Y-%m-%d'\n",
    "\n",
    "X_test = pd.read_csv('./all_test.csv')\n",
    "X_test = X_test.drop(['cons_last_month.1'], axis=1)\n",
    "raw_cols = list(X_test.columns)\n",
    "\n",
    "for i in id_todo:\n",
    "    tmp_df = test_todo[test_todo.id == i]\n",
    "    \n",
    "    # channel_sales\n",
    "    if tmp_df['channel_sales'].values[0] != 'nodata':\n",
    "        tmp_cha = [0] * len(channel)\n",
    "        try:\n",
    "            tmp_cha[channel.index(tmp_df['channel_sales'].values[0])] = 1\n",
    "        except ValueError as e:\n",
    "            pass\n",
    "    else:\n",
    "        tmp_cha = [0] * len(channel)\n",
    "    for idx in range(len(chan_cols)):\n",
    "        tmp_df[chan_cols[idx]] = [tmp_cha[idx]] \n",
    "    tmp_df = tmp_df.drop(['channel_sales'], axis=1)    \n",
    "    \n",
    "    # date_activ and date_end\n",
    "    tmp_df['date_activ'] = [(datetime.strptime(tmp_df['date_activ'].values[0], FMT) - datetime.strptime(start_date, FMT)).days]\n",
    "    tmp_df['date_end'] = [(datetime.strptime(tmp_df['date_end'].values[0], FMT) - datetime.strptime(start_date, FMT)).days]\n",
    "    \n",
    "    # date_modif_prod\n",
    "    if tmp_df['date_modif_prod'].values[0] != 'nodata':\n",
    "        tmp_df['date_modif_prod'] = [(datetime.strptime(tmp_df['date_modif_prod'].values[0], FMT) - datetime.strptime(start_date, FMT)).days]\n",
    "    else:\n",
    "        tmp_df['date_modif_prod'] = [train_df.date_modif_prod.mean()]\n",
    "    \n",
    "    # date_renewal\n",
    "    if tmp_df['date_renewal'].values[0] != 'nodata':\n",
    "        tmp_df['date_renewal'] = [(datetime.strptime(tmp_df['date_renewal'].values[0], FMT) - datetime.strptime(start_date, FMT)).days]\n",
    "    else:\n",
    "        tmp_df['date_renewal'] = [train_df.date_renewal.mean()]\n",
    "    \n",
    "    # origin_up\n",
    "    if tmp_df['origin_up'].values[0] != 'nodata':\n",
    "        tmp_origin = [0] * len(origin_up)\n",
    "        tmp_origin[origin_up.index(tmp_df['origin_up'].values[0])] = 1\n",
    "    else:\n",
    "        tmp_origin = [0] * len(origin_up)\n",
    "    for idx in range(len(origins_cols)):\n",
    "        tmp_df[origins_cols[idx]] = [tmp_origin[idx]]\n",
    "    tmp_df = tmp_df.drop(['origin_up'], axis=1)\n",
    "    \n",
    "        \n",
    "    # has_gas\n",
    "    if tmp_df['has_gas'].values[0] == 't':\n",
    "        tmp_df['has_gas'] = [1]\n",
    "    else:\n",
    "        tmp_df['has_gas'] = [0]\n",
    "        \n",
    "    # price\n",
    "    tmp_df['price_p1_var'] = [test_hist_todo[test_hist_todo.id == i]['price_p1_var'].mean()]\n",
    "    tmp_df['price_p2_var'] = [test_hist_todo[test_hist_todo.id == i]['price_p2_var'].mean()]\n",
    "    tmp_df['price_p3_var'] = [test_hist_todo[test_hist_todo.id == i]['price_p3_var'].mean()]\n",
    "    tmp_df['price_p1_fix'] = [test_hist_todo[test_hist_todo.id == i]['price_p1_fix'].mean()]\n",
    "    tmp_df['price_p2_fix'] = [test_hist_todo[test_hist_todo.id == i]['price_p2_fix'].mean()]\n",
    "    tmp_df['price_p3_fix'] = [test_hist_todo[test_hist_todo.id == i]['price_p3_fix'].mean()]\n",
    "    \n",
    "    # else\n",
    "    cols_todo = ['forecast_discount_energy', 'forecast_price_energy_p1', 'forecast_price_energy_p2', 'forecast_price_pow_p1', 'margin_gross_pow_ele', 'margin_net_pow_ele', 'net_margin','pow_max']\n",
    "    for col in cols_todo:\n",
    "        if tmp_df[col].values[0] == 'nodata':\n",
    "            tmp_df[col] = [train_df[col].mean()]\n",
    "    \n",
    "    tmp_df = tmp_df[raw_cols]\n",
    "    X_test = X_test.append(tmp_df, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.to_csv('./all_test_modified.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train final model and make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('./all_train.csv')\n",
    "train_gbm = train_df.drop(['id', 'cons_last_month.1'], axis=1)\n",
    "X_train = train_gbm.drop(['churn'], axis=1).as_matrix()\n",
    "y_train = train_gbm['churn'].as_matrix()\n",
    "X_resampled, y_resampled = TomekLinks().fit_sample(X_train, y_train)\n",
    "gbm = GradientBoostingClassifier(n_estimators=150, \\\n",
    "                                 learning_rate=0.2, \\\n",
    "                                 max_depth=3, \\\n",
    "                                 min_samples_leaf=1)\n",
    "gbm.fit(X_resampled, y_resampled)\n",
    "\n",
    "# Feed test data into our model\n",
    "X_test = pd.read_csv('./all_test_modified.csv')\n",
    "test_id = list(X_test.id)\n",
    "X_test_df = X_test.drop(['id', 'Unnamed: 0'], axis=1).as_matrix()\n",
    "y_test_pred = gbm.predict(X_test_df).tolist()\n",
    "y_test_pred_prob = [x[1] for x in gbm.predict_proba(X_test_df)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4024"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_cols = ['id', 'Churn_prediction', 'Churn_probability']\n",
    "results = [[test_id[idx], y_test_pred[idx], y_test_pred_prob[idx]] for idx in range(len(test_id))]\n",
    "results.sort(key=lambda x: x[2], reverse=True)\n",
    "\n",
    "with open ('../../ml_case_data/ml_case_test_output.csv', 'w') as f:\n",
    "    f.write(','.join(result_cols))\n",
    "    for line in results:\n",
    "        f.write('\\n')\n",
    "        f.write(','.join([str(x) for x in line]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply discount to churn clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply 20% discount to price columns\n",
    "X_disc = X_test.drop(['Unnamed: 0'], axis=1)\n",
    "disc_cols = ['price_p1_var', 'price_p2_var', 'price_p3_var', 'price_p1_fix', 'price_p2_fix', 'price_p3_fix']\n",
    "churn_id = [x[0] for x in results if x[1] == 1]\n",
    "for i in churn_id:\n",
    "    for col in disc_cols:\n",
    "        # dunno why this does not work\n",
    "        # X_disc[X_disc.id == i][col] = X_disc[X_disc.id == i][col].values[0] * 0.8\n",
    "        X_disc.loc[X_disc.id == i, col] = X_disc[X_disc.id == i][col].values[0] * 0.8\n",
    "\n",
    "# use our trained gbm model to predict\n",
    "X_disc_arr = X_disc.drop(['id'], axis=1).as_matrix()\n",
    "y_disc_pred = gbm.predict(X_disc_arr).tolist()\n",
    "y_disc_pred_prob = [x[1] for x in gbm.predict_proba(X_disc_arr)]\n",
    "\n",
    "results_disc = [[list(X_disc.id)[idx], y_disc_pred[idx], y_disc_pred_prob[idx]] for idx in range(len(list(X_disc.id)))]\n",
    "results_disc.sort(key=lambda x: x[2], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many customers will not churn after the discount?\n",
    "churn_id = [x[0] for x in results if x[1] == 1]\n",
    "nochurn_id = [x[0] for x in results if x[1] == 0]\n",
    "prob_dict = {x[0]:x[2] for x in results}\n",
    "\n",
    "churn_disc_id = [x[0] for x in results_disc if x[1] == 1]\n",
    "nochurn_disc_id = [x[0] for x in results_disc if x[1] == 0]\n",
    "disc_prob_dict = {x[0]:x[2] for x in results_disc}\n",
    "\n",
    "# customer who turns from churn to nochurn and, nochurn to churn\n",
    "churn2no_id = [x for x in churn_id if x in nochurn_disc_id]\n",
    "no2churn_id = [x for x in nochurn_id if x in churn_disc_id]\n",
    "\n",
    "# churn probabilities of churn customer before and after\n",
    "churn_prob = []\n",
    "churn_disc_prob = []\n",
    "for i in churn_id:\n",
    "    churn_prob.append(prob_dict[i])\n",
    "    churn_disc_prob.append(disc_prob_dict[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Churn customers:  38\n",
      "Churn to Nochurn:  18\n",
      "Nochurn to Churn:  0\n",
      "Mean churn probability of Churn customer:  0.6328473199728\n",
      "Mean churn probability of Churn customer (discount):  0.5467513802574293\n",
      "Mean churn probability:  0.09519933325798939\n",
      "Mean churn probability (discount):  0.09438630003006093\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "prob_diff = [churn_prob[idx] - churn_disc_prob[idx] for idx in range(len(churn_prob))]\n",
    "prob_diff = np.array(prob_diff)\n",
    "\n",
    "print ('Number of Churn customers: ', len(churn_id))\n",
    "print ('Churn to Nochurn: ', len(churn2no_id))\n",
    "print ('Nochurn to Churn: ', len(no2churn_id))\n",
    "print ('Mean churn probability of Churn customer: ', np.array(churn_prob).mean())\n",
    "print ('Mean churn probability of Churn customer (discount): ', np.array(churn_disc_prob).mean())\n",
    "print ('Mean churn probability: ', np.array(list(prob_dict.values())).mean())\n",
    "print ('Mean churn probability (discount): ', np.array(list(disc_prob_dict.values())).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.91291811,  0.48873494, -0.03590337, -0.03590337, -0.03121291,\n",
       "        0.00478556,  0.00823381,  0.18131058,  0.14686485, -0.07432545,\n",
       "        0.25811355, -0.22360476, -0.06857641,  0.22821947, -0.20310275,\n",
       "       -0.03764857,  0.18573855,  0.57257907,  0.16384057, -0.27008472,\n",
       "       -0.38433694, -0.28583721,  0.23829282, -0.09658989,  0.18366542,\n",
       "       -0.15812541,  0.19269748,  0.14798905,  0.42985412, -0.08741774,\n",
       "        0.05007548,  0.23190981,  0.16366733, -0.02945533,  0.25329494,\n",
       "       -0.10044093,  0.2053094 ,  0.14611655])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob_diff"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
